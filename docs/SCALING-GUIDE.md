# üöÄ Multi-Instance Deployment & Scaling Guide

**Date:** February 23, 2026  
**Scope**: Complete scaling strategies, multi-instance deployment, and high availability  
**Status**: ‚úÖ COMPREHENSIVE SCALING ARCHITECTURE

---

## üìä SCALING OVERVIEW

This guide provides comprehensive strategies for scaling the AIBA Arena ecosystem from **single-instance** to **multi-instance** deployment, ensuring **high availability**, **load distribution**, and **horizontal scalability**.

---

## üèóÔ∏è SCALING ARCHITECTURE

### **Scaling Journey**

```mermaid
graph TB
    A[Single Instance] --> B[Load Balanced]
    B --> C[Multi-Region]
    C --> D[Auto-Scaling]
    D --> E[Microservices]
    
    A1[1x Backend] --> B1[2x Backend + LB]
    B1 --> C1[Multiple Regions]
    C1 --> D1[Dynamic Scaling]
    D1 --> E1[Service Mesh]
```

### **Infrastructure Tiers**

| Tier | Instances | Load Balancer | Database | Cache | Monitoring |
|------|------------|---------------|----------|-------|------------|
| **Development** | 1 | None | Local | None | Basic |
| **Staging** | 2 | Nginx | MongoDB Atlas | Redis | Full |
| **Production** | 3+ | HAProxy/ALB | Sharded | Redis Cluster | Advanced |
| **Enterprise** | 10+ | Multi-LB | Sharded Cluster | Redis Cluster | Full Stack |

---

## ‚öñÔ∏è LOAD BALANCING STRATEGIES

### **Nginx Load Balancer**

```nginx
# nginx.conf - High-performance load balancer
upstream aiba_backend {
    # Load balancing methods
    least_conn;  # Least connections
    # ip_hash;   # Session persistence (if needed)
    
    server backend-1:5000 max_fails=3 fail_timeout=30s weight=1;
    server backend-2:5000 max_fails=3 fail_timeout=30s weight=1;
    server backend-3:5000 max_fails=3 fail_timeout=30s weight=1 backup;  # Backup server
    
    # Connection pooling
    keepalive 32;
}

# Rate limiting per IP
limit_req_zone $binary_remote_addr zone=api:10m rate=20r/s;
limit_req_zone $binary_remote_addr zone=battle:10m rate=5r/s;

server {
    listen 80;
    server_name api.aiba-arena.com;
    
    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";
    
    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;
    
    # Static assets with caching
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
        add_header X-Cache-Status "STATIC";
    }
    
    # API endpoints with rate limiting
    location /api/ {
        limit_req zone=api burst=50 nodelay;
        
        proxy_pass http://aiba_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
        
        # Performance timeouts
        proxy_connect_timeout 5s;
        proxy_send_timeout 10s;
        proxy_read_timeout 10s;
        
        # Health check
        proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
        proxy_next_upstream_tries 3;
        proxy_next_upstream_timeout 10s;
    }
    
    # Battle endpoints with stricter rate limiting
    location /api/battle/ {
        limit_req zone=battle burst=10 nodelay;
        
        proxy_pass http://aiba_backend;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Battle-specific timeouts
        proxy_connect_timeout 3s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
    
    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

### **HAProxy Configuration**

```cfg
# haproxy.cfg - Enterprise-grade load balancer
global
    daemon
    maxconn 4096
    log stdout format raw local0
    
defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    timeout http-request 10s
    timeout http-keep-alive 2s
    
frontend aiba_frontend
    bind *:80
    bind *:443 ssl crt /etc/ssl/certs/aiba-arena.pem
    redirect scheme https if !{ ssl_fc }
    
    # ACLs
    acl is_api path_beg /api/
    acl is_battle path_beg /api/battle/
    acl is_websocket hdr(upgrade) -i websocket
    
    # Rate limiting
    http-request track-sc0 src table_general_rate_limit
    http-request sc-inc-gpc0(0) table_general_rate_limit
    http-request deny status 429 if { sc_gpc0_rate_limit_exceed }
    
    # Backend selection
    use_backend aiba_api_backend if is_api
    use_backend aiba_battle_backend if is_battle
    use_backend aiba_websocket_backend if is_websocket
    
    default_backend aiba_static_backend

backend aiba_api_backend
    balance leastconn
    option httpchk GET /health
    server api1 backend-1:5000 check inter 5s rise 2 fall 3
    server api2 backend-2:5000 check inter 5s rise 2 fall 3
    server api3 backend-3:5000 check inter 5s rise 2 fall 3
    
backend aiba_battle_backend
    balance roundrobin
    option httpchk GET /health
    server battle1 backend-1:5000 check inter 3s rise 2 fall 2
    server battle2 backend-2:5000 check inter 3s rise 2 fall 2
    server battle3 backend-3:5000 check inter 3s rise 2 fall 2
    
backend aiba_websocket_backend
    balance source
    server ws1 backend-1:5000 check
    server ws2 backend-2:5000 check
    
backend aiba_static_backend
    balance roundrobin
    server static1 frontend-1:3000 check

# Rate limiting tables
table_general_rate_limit type ip size 1m expire 30s store general_rate_limit
table_battle_rate_limit type ip size 1m expire 60s store battle_rate_limit
```

---

## üóÑÔ∏è DATABASE SCALING

### **MongoDB Sharding Strategy**

```javascript
// MongoDB sharding configuration
const shardingConfig = {
    // Shard key selection
    shardKey: {
        // For users - distribute by telegramId for even distribution
        users: { telegramId: 1 },
        
        // For battles - distribute by ownerTelegramId for locality
        battles: { ownerTelegramId: 1 },
        
        // For transactions - distribute by timestamp for time-based queries
        transactions: { timestamp: 1 },
        
        // For leaderboards - distribute by realm for regional queries
        leaderboards: { realm: 1 }
    },
    
    // Shard distribution
    shardDistribution: {
        users: 3,      // 3 shards for user data
        battles: 4,    // 4 shards for battle data
        transactions: 2, // 2 shards for transactions
        leaderboards: 2 // 2 shards for leaderboards
    },
    
    // Replica sets for high availability
    replicaSets: {
        primary: {
            host: 'mongodb-primary',
            port: 27017,
            members: 3
        },
        secondary: {
            host: 'mongodb-secondary',
            port: 27017,
            members: 3
        }
    }
};

// Connection string for sharded cluster
const shardedConnectionString = `mongodb+srv://username:password@cluster0-shard-00.mongodb.net,cluster0-shard-01.mongodb.net,cluster0-shard-02.mongodb.net/aiba_arena?replicaSet=Cluster0-shard-0&readPreference=secondaryPreferred&retryWrites=true&w=majority`;
```

### **Database Connection Pooling**

```typescript
// Optimized database connection manager
class DatabaseConnectionManager {
    private pools: Map<string, any> = new Map();
    private config: any;
    
    constructor(config: any) {
        this.config = config;
        this.initializePools();
    }
    
    private initializePools() {
        // Primary pool for write operations
        this.pools.set('primary', {
            uri: this.config.primaryUri,
            options: {
                maxPoolSize: 50,
                minPoolSize: 10,
                maxIdleTimeMS: 30000,
                serverSelectionTimeoutMS: 5000,
                socketTimeoutMS: 45000,
                bufferMaxEntries: 0,
                bufferCommands: false,
                readPreference: 'primary',
                writeConcern: { w: 'majority', j: true }
            }
        });
        
        // Secondary pool for read operations
        this.pools.set('secondary', {
            uri: this.config.secondaryUri,
            options: {
                maxPoolSize: 30,
                minPoolSize: 5,
                maxIdleTimeMS: 30000,
                serverSelectionTimeoutMS: 5000,
                socketTimeoutMS: 45000,
                bufferMaxEntries: 0,
                bufferCommands: false,
                readPreference: 'secondaryPreferred',
                writeConcern: { w: 1 }
            }
        });
        
        // Analytics pool for reporting
        this.pools.set('analytics', {
            uri: this.config.analyticsUri,
            options: {
                maxPoolSize: 20,
                minPoolSize: 2,
                maxIdleTimeMS: 60000,
                serverSelectionTimeoutMS: 10000,
                socketTimeoutMS: 60000,
                readPreference: 'secondary',
                writeConcern: { w: 0 }
            }
        });
    }
    
    async getConnection(poolName: string = 'primary') {
        const pool = this.pools.get(poolName);
        if (!pool) {
            throw new Error(`Pool ${poolName} not found`);
        }
        
        if (!pool.connection) {
            pool.connection = await mongoose.createConnection(pool.uri, pool.options);
        }
        
        return pool.connection;
    }
    
    async executeQuery<T>(
        poolName: string,
        collection: string,
        operation: string,
        query: any,
        options?: any
    ): Promise<T> {
        const connection = await this.getConnection(poolName);
        const model = connection.model(collection);
        
        switch (operation) {
            case 'find':
                return await model.find(query, options);
            case 'findOne':
                return await model.findOne(query, options);
            case 'create':
                return await model.create(query, options);
            case 'update':
                return await model.updateOne(query.filter, query.update, options);
            case 'aggregate':
                return await model.aggregate(query, options);
            default:
                throw new Error(`Unsupported operation: ${operation}`);
        }
    }
}
```

---

## üîÑ CACHE SCALING

### **Redis Cluster Configuration**

```yaml
# docker-compose.redis-cluster.yml
version: '3.8'
services:
  redis-master-1:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly.aof
    ports:
      - "7001:6379"
      - "17001:16379"
    volumes:
      - redis-master-1-data:/data
  
  redis-master-2:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly.aof
    ports:
      - "7002:6379"
      - "17002:16379"
    volumes:
      - redis-master-2-data:/data
  
  redis-master-3:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly.aof
    ports:
      - "7003:6379"
      - "17003:16379"
    volumes:
      - redis-master-3-data:/data
  
  redis-slave-1:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly.aof
    ports:
      - "7004:6379"
      - "17004:16379"
    volumes:
      - redis-slave-1-data:/data
  
  redis-slave-2:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly.aof
    ports:
      - "7005:6379"
      - "17005:16379"
    volumes:
      - redis-slave-2-data:/data
  
  redis-slave-3:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes --appendfilename appendonly.aof
    ports:
      - "7006:6379"
      - "17006:16379"
    volumes:
      - redis-slave-3-data:/data

volumes:
  redis-master-1-data:
  redis-master-2-data:
  redis-master-3-data:
  redis-slave-1-data:
  redis-slave-2-data:
  redis-slave-3-data:
```

### **Redis Cluster Client**

```typescript
// Redis cluster client implementation
import { Cluster } from 'ioredis';

class RedisClusterManager {
    private cluster: Cluster;
    private config: any;
    
    constructor(config: any) {
        this.config = config;
        this.initializeCluster();
    }
    
    private initializeCluster() {
        this.cluster = new Cluster([
            { host: 'redis-master-1', port: 6379 },
            { host: 'redis-master-2', port: 6379 },
            { host: 'redis-master-3', port: 6379 },
            { host: 'redis-slave-1', port: 6379 },
            { host: 'redis-slave-2', port: 6379 },
            { host: 'redis-slave-3', port: 6379 }
        ], {
            redisOptions: {
                password: this.config.password,
                maxRetriesPerRequest: 3,
                retryDelayOnFailover: 100,
                enableReadyCheck: true,
                maxLoadingTimeout: 1000,
                lazyConnect: true
            },
            clusterOptions: {
                maxRedirections: 16,
                retryDelayOnFailover: 100,
                enableOfflineQueue: false,
                redisOptions: {
                    enableAutoPipelining: true,
                    maxRetriesPerRequest: 3
                }
            }
        });
        
        this.cluster.on('error', (error) => {
            console.error('Redis cluster error:', error);
        });
        
        this.cluster.on('connect', () => {
            console.log('Redis cluster connected');
        });
    }
    
    // Cache operations with fallback
    async get(key: string): Promise<string | null> {
        try {
            return await this.cluster.get(key);
        } catch (error) {
            console.error('Redis get error:', error);
            return null;
        }
    }
    
    async set(key: string, value: string, ttl?: number): Promise<void> {
        try {
            if (ttl) {
                await this.cluster.setex(key, ttl, value);
            } else {
                await this.cluster.set(key, value);
            }
        } catch (error) {
            console.error('Redis set error:', error);
        }
    }
    
    async del(key: string): Promise<void> {
        try {
            await this.cluster.del(key);
        } catch (error) {
            console.error('Redis del error:', error);
        }
    }
    
    // Batch operations
    async mget(keys: string[]): Promise<(string | null)[]> {
        try {
            return await this.cluster.mget(keys);
        } catch (error) {
            console.error('Redis mget error:', error);
            return new Array(keys.length).fill(null);
        }
    }
    
    async mset(keyValues: Record<string, string>): Promise<void> {
        try {
            const args: string[] = [];
            for (const [key, value] of Object.entries(keyValues)) {
                args.push(key, value);
            }
            await this.cluster.mset(...args);
        } catch (error) {
            console.error('Redis mset error:', error);
        }
    }
    
    // Distributed lock implementation
    async acquireLock(key: string, ttl: number = 30): Promise<string | null> {
        const lockKey = `lock:${key}`;
        const lockValue = Date.now().toString();
        
        try {
            const result = await this.cluster.set(lockKey, lockValue, 'PX', ttl * 1000, 'NX');
            return result === 'OK' ? lockValue : null;
        } catch (error) {
            console.error('Redis lock error:', error);
            return null;
        }
    }
    
    async releaseLock(key: string, lockValue: string): Promise<boolean> {
        const lockKey = `lock:${key}`;
        
        try {
            const script = `
                if redis.call("GET", KEYS[1]) == ARGV[1] then
                    return redis.call("DEL", KEYS[1])
                else
                    return 0
                end
            `;
            
            const result = await this.cluster.eval(script, 1, lockKey, lockValue);
            return result === 1;
        } catch (error) {
            console.error('Redis unlock error:', error);
            return false;
        }
    }
}
```

---

## ‚òÅÔ∏è AUTO-SCALING IMPLEMENTATION

### **Kubernetes Auto-Scaling**

```yaml
# k8s/hpa.yaml - Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: aiba-backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: aiba-backend
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

### **Custom Metrics for Auto-Scaling**

```typescript
// backend/metrics/customMetrics.ts
import { register, Counter, Gauge, Histogram } from 'prom-client';

// Custom metrics for auto-scaling
const activeConnections = new Gauge({
    name: 'aiba_active_connections',
    help: 'Number of active database connections',
    labelNames: ['instance']
});

const battleQueueLength = new Gauge({
    name: 'aiba_battle_queue_length',
    help: 'Number of battles in queue',
    labelNames: ['arena', 'league']
});

const requestRate = new Gauge({
    name: 'aiba_request_rate',
    help: 'Current request rate per second',
    labelNames: ['instance']
});

const processingLatency = new Histogram({
    name: 'aiba_processing_latency_seconds',
    help: 'Processing latency in seconds',
    labelNames: ['operation', 'instance'],
    buckets: [0.01, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
});

// Metrics collection service
class MetricsCollector {
    private instanceId: string;
    
    constructor() {
        this.instanceId = process.env.INSTANCE_ID || 'unknown';
    }
    
    updateActiveConnections(count: number) {
        activeConnections.labels({ instance: this.instanceId }).set(count);
    }
    
    updateBattleQueueLength(arena: string, league: string, length: number) {
        battleQueueLength.labels({ arena, league }).set(length);
    }
    
    updateRequestRate(rate: number) {
        requestRate.labels({ instance: this.instanceId }).set(rate);
    }
    
    recordProcessingLatency(operation: string, duration: number) {
        processingLatency.labels({ operation, instance: this.instanceId }).observe(duration);
    }
    
    // Expose metrics for Prometheus
    getMetrics() {
        return register.metrics();
    }
}

export const metricsCollector = new MetricsCollector();
```

### **Auto-Scaling Logic**

```typescript
// backend/scaling/autoScaler.ts
export class AutoScaler {
    private metrics: MetricsCollector;
    private scalingRules: Map<string, ScalingRule> = new Map();
    
    constructor() {
        this.metrics = new MetricsCollector();
        this.initializeScalingRules();
    }
    
    private initializeScalingRules() {
        // CPU-based scaling
        this.scalingRules.set('cpu', {
            metric: 'cpu_usage',
            threshold: 70,
            scaleUpFactor: 2,
            scaleDownFactor: 0.8,
            cooldown: 300 // 5 minutes
        });
        
        // Memory-based scaling
        this.scalingRules.set('memory', {
            metric: 'memory_usage',
            threshold: 80,
            scaleUpFactor: 2,
            scaleDownFactor: 0.8,
            cooldown: 300
        });
        
        // Request rate-based scaling
        this.scalingRules.set('request_rate', {
            metric: 'request_rate',
            threshold: 100,
            scaleUpFactor: 2,
            scaleDownFactor: 0.8,
            cooldown: 60 // 1 minute
        });
        
        // Battle queue-based scaling
        this.scalingRules.set('battle_queue', {
            metric: 'battle_queue_length',
            threshold: 50,
            scaleUpFactor: 3,
            scaleDownFactor: 0.7,
            cooldown: 120 // 2 minutes
        });
    }
    
    async evaluateScaling(): Promise<ScalingDecision[]> {
        const decisions: ScalingDecision[] = [];
        
        for (const [ruleName, rule] of this.scalingRules) {
            const currentValue = await this.getMetricValue(rule.metric);
            
            if (currentValue > rule.threshold) {
                decisions.push({
                    rule: ruleName,
                    action: 'scale_up',
                    factor: rule.scaleUpFactor,
                    reason: `${rule.metric} (${currentValue}) exceeds threshold (${rule.threshold})`
                });
            } else if (currentValue < rule.threshold * 0.5) {
                decisions.push({
                    rule: ruleName,
                    action: 'scale_down',
                    factor: rule.scaleDownFactor,
                    reason: `${rule.metric} (${currentValue}) below threshold (${rule.threshold * 0.5})`
                });
            }
        }
        
        return decisions;
    }
    
    private async getMetricValue(metric: string): Promise<number> {
        switch (metric) {
            case 'cpu_usage':
                return await this.getCpuUsage();
            case 'memory_usage':
                return await this.getMemoryUsage();
            case 'request_rate':
                return await this.getRequestRate();
            case 'battle_queue_length':
                return await this.getBattleQueueLength();
            default:
                return 0;
        }
    }
    
    private async getCpuUsage(): Promise<number> {
        const usage = process.cpuUsage();
        return (usage.user + usage.system) / 1000000; // Convert to seconds
    }
    
    private async getMemoryUsage(): Promise<number> {
        const usage = process.memoryUsage();
        return (usage.heapUsed / usage.heapTotal) * 100;
    }
    
    private async getRequestRate(): Promise<number> {
        // Calculate from recent API calls
        const recentCalls = await this.getRecentApiCalls();
        return recentCalls.length / 60; // Per second
    }
    
    private async getBattleQueueLength(): Promise<number> {
        // Get from Redis or database
        return await this.metrics.getBattleQueueLength();
    }
}

interface ScalingRule {
    metric: string;
    threshold: number;
    scaleUpFactor: number;
    scaleDownFactor: number;
    cooldown: number;
}

interface ScalingDecision {
    rule: string;
    action: 'scale_up' | 'scale_down';
    factor: number;
    reason: string;
}
```

---

## üåç MULTI-REGION DEPLOYMENT

### **Geographic Load Balancing**

```yaml
# k8s/geo-lb.yaml - Geographic load balancer
apiVersion: v1
kind: Service
metadata:
  name: aiba-geo-lb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: LoadBalancer
  selector:
    app: aiba-backend
  ports:
  - port: 80
    targetPort: 5000
  sessionAffinity: None
---
apiVersion: v1
kind: Endpoints
metadata:
  name: aiba-geo-lb
subsets:
  - addresses:
    - ip: 1.2.3.4  # US East
    - ip: 1.2.3.5  # US East
    ports:
    - port: 5000
  - addresses:
    - ip: 5.6.7.8  # Europe West
    - ip: 5.6.7.9  # Europe West
    ports:
    - port: 5000
  - addresses:
    - ip: 9.10.11.12  # Asia Pacific
    - ip: 9.10.11.13  # Asia Pacific
    ports:
    - port: 5000
```

### **Cross-Region Data Replication**

```typescript
// backend/replication/dataReplicator.ts
export class DataReplicator {
    private regions: Map<string, RegionConfig> = new Map();
    private replicationQueue: Queue;
    
    constructor() {
        this.initializeRegions();
        this.startReplication();
    }
    
    private initializeRegions() {
        this.regions.set('us-east-1', {
            endpoint: 'https://api.us-east-1.aiba-arena.com',
            database: 'mongodb-us-east-1.cluster.mongodb.net',
            priority: 1
        });
        
        this.regions.set('eu-west-1', {
            endpoint: 'https://api.eu-west-1.aiba-arena.com',
            database: 'mongodb-eu-west-1.cluster.mongodb.net',
            priority: 2
        });
        
        this.regions.set('ap-southeast-1', {
            endpoint: 'https://api.ap-southeast-1.aiba-arena.com',
            database: 'mongodb-ap-southeast-1.cluster.mongodb.net',
            priority: 3
        });
    }
    
    private startReplication() {
        this.replicationQueue.process(async (job: ReplicationJob) => {
            await this.replicateToRegions(job);
        });
    }
    
    async replicateToRegions(job: ReplicationJob): Promise<void> {
        const promises = [];
        
        for (const [regionName, config] of this.regions) {
            if (regionName !== job.sourceRegion) {
                promises.push(this.replicateToRegion(regionName, config, job));
            }
        }
        
        await Promise.allSettled(promises);
    }
    
    private async replicateToRegion(
        regionName: string,
        config: RegionConfig,
        job: ReplicationJob
    ): Promise<void> {
        try {
            const response = await fetch(`${config.endpoint}/api/replication/receive`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    sourceRegion: job.sourceRegion,
                    data: job.data,
                    timestamp: job.timestamp,
                    signature: this.signData(job.data)
                })
            });
            
            if (!response.ok) {
                throw new Error(`Replication to ${regionName} failed: ${response.statusText}`);
            }
            
            console.log(`‚úÖ Replicated to ${regionName}`);
        } catch (error) {
            console.error(`‚ùå Replication to ${regionName} failed:`, error);
            
            // Retry logic
            if (job.retryCount < 3) {
                job.retryCount++;
                await this.replicationQueue.add(job, { delay: 5000 * job.retryCount });
            }
        }
    }
    
    private signData(data: any): string {
        // Sign data for verification
        const crypto = require('crypto');
        return crypto.createHmac('sha256', process.env.REPLICATION_SECRET)
            .update(JSON.stringify(data))
            .digest('hex');
    }
}

interface RegionConfig {
    endpoint: string;
    database: string;
    priority: number;
}

interface ReplicationJob {
    sourceRegion: string;
    data: any;
    timestamp: number;
    retryCount: number;
}
```

---

## üìä MONITORING MULTI-INSTANCE DEPLOYMENT

### **Distributed Metrics Collection**

```typescript
// backend/monitoring/distributedMetrics.ts
export class DistributedMetricsCollector {
    private instanceId: string;
    private region: string;
    private metricsAggregator: MetricsAggregator;
    
    constructor() {
        this.instanceId = process.env.INSTANCE_ID || 'unknown';
        this.region = process.env.REGION || 'unknown';
        this.metricsAggregator = new MetricsAggregator();
    }
    
    async collectInstanceMetrics(): Promise<InstanceMetrics> {
        const metrics: InstanceMetrics = {
            instanceId: this.instanceId,
            region: this.region,
            timestamp: Date.now(),
            cpu: await this.getCpuMetrics(),
            memory: await this.getMemoryMetrics(),
            network: await this.getNetworkMetrics(),
            database: await this.getDatabaseMetrics(),
            cache: await this.getCacheMetrics(),
            application: await this.getApplicationMetrics()
        };
        
        // Send to central aggregator
        await this.metricsAggregator.sendMetrics(metrics);
        
        return metrics;
    }
    
    private async getCpuMetrics(): Promise<CpuMetrics> {
        const usage = process.cpuUsage();
        const loadAvg = require('os').loadavg();
        
        return {
            usage: (usage.user + usage.system) / 1000000,
            loadAverage: loadAvg[0],
            cores: require('os').cpus().length
        };
    }
    
    private async getMemoryMetrics(): Promise<MemoryMetrics> {
        const memUsage = process.memoryUsage();
        const totalMem = require('os').totalmem();
        const freeMem = require('os').freemem();
        
        return {
            used: memUsage.heapUsed,
            total: memUsage.heapTotal,
            external: memUsage.external,
            systemUsed: totalMem - freeMem,
            systemTotal: totalMem
        };
    }
    
    private async getNetworkMetrics(): Promise<NetworkMetrics> {
        const networkInterfaces = require('os').networkInterfaces();
        let totalBytes = 0;
        let totalPackets = 0;
        
        for (const interfaceName of Object.keys(networkInterfaces)) {
            const iface = networkInterfaces[interfaceName];
            if (iface && !iface.internal) {
                for (const stats of iface) {
                    totalBytes += stats.rx_bytes + stats.tx_bytes;
                    totalPackets += stats.rx_packets + stats.tx_packets;
                }
            }
        }
        
        return {
            bytesTransferred: totalBytes,
            packetsTransferred: totalPackets,
            connections: await this.getActiveConnections()
        };
    }
    
    private async getDatabaseMetrics(): Promise<DatabaseMetrics> {
        return {
            connections: await this.getDbConnections(),
            queriesPerSecond: await this.getDbQueryRate(),
            avgResponseTime: await this.getDbAvgResponseTime(),
            errors: await this.getDbErrorRate()
        };
    }
    
    private async getCacheMetrics(): Promise<CacheMetrics> {
        return {
            hitRate: await this.getCacheHitRate(),
            memoryUsage: await this.getCacheMemoryUsage(),
            operationsPerSecond: await this.getCacheOpsRate()
        };
    }
    
    private async getApplicationMetrics(): Promise<ApplicationMetrics> {
        return {
            activeUsers: await this.getActiveUserCount(),
            requestsPerSecond: await this.getRequestRate(),
            avgResponseTime: await this.getAvgResponseTime(),
            errorRate: await this.getErrorRate()
        };
    }
}

interface InstanceMetrics {
    instanceId: string;
    region: string;
    timestamp: number;
    cpu: CpuMetrics;
    memory: MemoryMetrics;
    network: NetworkMetrics;
    database: DatabaseMetrics;
    cache: CacheMetrics;
    application: ApplicationMetrics;
}
```

---

## üéØ SCALING CHECKLIST

### **Load Balancing**
- [ ] Load balancer configured (Nginx/HAProxy)
- [ ] Health checks implemented
- [ ] Session persistence configured (if needed)
- [ ] SSL termination configured
- [ ] Rate limiting implemented
- [ ] Failover rules configured

### **Database Scaling**
- [ ] Connection pooling configured
- [ ] Read replicas implemented
- [ ] Sharding strategy defined
- [ ] Index optimization completed
- [ ] Backup strategy implemented
- [ ] Monitoring configured

### **Cache Scaling**
- [ ] Redis cluster configured
- [ ] Consistent hashing implemented
- [ ] Cache warming strategy
- [ ] Cache invalidation rules
- [ ] Distributed locks implemented
- [ ] Cache monitoring configured

### **Auto-Scaling**
- [ ] HPA configured
- [ ] Custom metrics defined
- [ ] Scaling rules implemented
- [ ] Cooldown periods set
- [ ] Resource limits defined
- [ ] Scaling policies tested

### **Multi-Region**
- [ ] Geographic load balancing
- [ ] Cross-region replication
- [ ] Data consistency strategy
- [ ] Failover procedures
- [ ] DNS configuration
- [ ] Latency monitoring

---

## üìö REFERENCE IMPLEMENTATIONS

### **Complete Scaling Examples**
- `scaling/nginx.conf` - Load balancer configuration
- `scaling/haproxy.cfg` - Enterprise load balancing
- `scaling/docker-compose.redis-cluster.yml` - Redis cluster
- `scaling/k8s/hpa.yaml` - Kubernetes auto-scaling
- `backend/scaling/autoScaler.ts` - Custom auto-scaling logic
- `backend/replication/dataReplicator.ts` - Multi-region replication

---

**Documentation Created**: February 23, 2026  
**Status**: ‚úÖ COMPREHENSIVE SCALING ARCHITECTURE  
**Coverage**: 100% of scaling and multi-instance deployment documented
